import { NextRequest, NextResponse } from "next/server";
import { Pinecone } from "@pinecone-database/pinecone";
import { ZhipuAI } from "zhipuai-sdk-nodejs-v4";

// å®šä¹‰AIå“åº”çš„æ¥å£
interface AIResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
}

/**
 * å‘é‡æ£€ç´¢API - å®Œå…¨å¼€æ”¾è®¿é—® (å·²ç™»å½•å’Œæœªç™»å½•ç”¨æˆ·å‡å¯ä½¿ç”¨)
 * ç”¨äºæ ¹æ®ç”¨æˆ·æŸ¥è¯¢æœç´¢ç›¸å…³æ¡ˆä¾‹
 */
export async function GET(req: NextRequest) {
  try {
    console.log("ğŸ” Vector search request received");
    const pc = new Pinecone({ apiKey: process.env.PINECONE_API_KEY! });
    const mynamespace = pc
      .index("finalindex", process.env.HOST_ADD!)
      .namespace("caselist");
    const ai = new ZhipuAI({ apiKey: process.env.AI_API_KEY });
    const searchString = req.nextUrl.searchParams.get("search");
    
    if (!searchString) {
      return NextResponse.json(
        { error: "Search string is required" },
        { status: 400 },
      );
    }
    
    console.log(`ğŸ“ Search query: ${searchString}`);
    const myaiResponse = await ai.createEmbeddings({
      input: searchString,
      model: "embedding-3",
      encodingFormat: "float",
      user: "sfd",
      sensitiveWordCheck: {
        check: true,
        replace: true,
        replaceWith: "***",
      },
    });
    
    // è·å–åŸå§‹å‘é‡å¹¶è°ƒæ•´ç»´åº¦ä»¥åŒ¹é…Pineconeç´¢å¼•
    const originalVector = myaiResponse.data[0].embedding;
    const adjustedVector = originalVector.slice(0, 1536); // æˆªå–å‰1536ç»´
    
    const queryResponse = await mynamespace.query({
      vector: adjustedVector,
      topK: 5,
      includeValues: true,
      includeMetadata: true,
    });
    const filteredMatches = queryResponse.matches.filter(
      (match) => (match?.score ?? 0) >= 0.0,
    );
    const recordDetails = filteredMatches.map((match) => ({
      title: match.metadata?.title,
      link: match.metadata?.link,
    }));
    const recordDetailsForAI = filteredMatches.map((match) => ({
      title: match.metadata?.title,
    }));
    const aiMessageContent = `ä»¥ä¸‹æ˜¯5ä¸ªäº‹ä¾‹: ${recordDetailsForAI.map((detail) => `æ ‡é¢˜: ${detail.title}`).join(";")}ã€‚è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜: "${searchString}"ã€‚è¯·åœ¨100å­—å†…è§£é‡Šè¿™äº”ä¸ªäº‹ä¾‹æ˜¯å¦‚ä½•è§£ç­”ç”¨æˆ·çš„é—®é¢˜çš„`;
    console.log("aiMessageContent:" + aiMessageContent);
    const aiResponse = (await ai.createCompletions({
      model: process.env.AI_MODEL || "glm-4-flashx",
      messages: [
        { role: "system", content: "è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ï¼Œ" },
        {
          role: "user",
          content: aiMessageContent,
        },
      ],
    })) as AIResponse;
    console.log("content:" + aiResponse.choices[0].message.content);
    const aiMessage =
      aiResponse.choices?.[0]?.message?.content || "No response from AI";
    
    console.log("âœ… Vector search completed successfully");
    return NextResponse.json({ cases: recordDetails, data: aiMessage });
  } catch (error) {
    console.error("âŒ Error fetching cases:", error);
    return NextResponse.json(
      { error: "Failed to fetch cases" },
      { status: 500 },
    );
  }
}
